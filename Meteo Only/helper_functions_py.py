# -*- coding: utf-8 -*-
"""helper_functions.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j99Y8L9M3zk2P8tcXO6la3NVV8CTZcJA
"""

import os
import pandas as pd
import torch
from torch.utils.data import Dataset
import numpy as np
from torch.utils.data import Subset
import seaborn as sns
import matplotlib.pyplot as plt
def save_labels_to_excel(dataset, output_directory):
    """
    Save labels to an Excel file.

    Arguments:
        dataset (GHIDataset): The dataset instance.
        output_directory (str): Directory to save the output file.
    """
    os.makedirs(output_directory, exist_ok=True)

    # Get labels
    _, labels = dataset[:]

    # Create a DataFrame
    labels_df = pd.DataFrame(data={'labels': labels.numpy()})

    # Save to Excel file
    labels_df.to_excel(os.path.join(output_directory, 'labels_only_meteo.xlsx'), index=False)

def save_meteo_to_excel(dataset, output_directory):
    """
    Save meteo data to an Excel file.

    Arguments:
        dataset (GHIDataset): The dataset instance.
        output_directory (str): Directory to save the output file.
    """
    os.makedirs(output_directory, exist_ok=True)

    # Get meteo data
    meteo_data, _ = dataset[:]

    # Create a DataFrame
    meteo_df = pd.DataFrame(data=meteo_data.numpy(), columns=dataset.meteo_columns)

    # Save to Excel file
    meteo_df.to_excel(os.path.join(output_directory, 'meteo_data_only.xlsx'), index=False)

def split_at_locations(dataset, start_indices, end_indices):
    """
    This function extracts the training set and three validation sets from the dataset
    based on given start and end indices.

    Arguments:
        dataset      (torch.utils.data.Dataset): The input dataset.
        start_indices (list): List of start indices for validation sets.
        end_indices   (list): List of end indices for validation sets.

    Returns:
        Tuple[torch.utils.data.Subset, List[torch.utils.data.Subset]]:
            - The training set.
            - List of three validation sets.
    """
    dataset_idx = np.arange(len(dataset))
    training_idx = []

    # Extract training set indices by removing validation set indices
    for start_idx, end_idx in zip(start_indices, end_indices):
        training_idx.extend(range(max(0, start_idx), min(len(dataset), end_idx)))

    training_set = Subset(dataset, np.setdiff1d(dataset_idx, training_idx))
    validation_sets = [Subset(dataset, range(start_idx, end_idx)) for start_idx, end_idx in zip(start_indices, end_indices)]

    return training_set, validation_sets

def find_indices_by_timestamps(dataset, target_timestamps):
    """
    Find the indices of the data with specific timestamps in the given dataset.

    Parameters:
    - dataset (GHIDataset): The dataset to search in.
    - target_timestamps (list of tuples): List of timestamps to search for in the format (day, month, year, hour, minute).

    Returns:
    - List of indices corresponding to the specified timestamps.
    """
    indices = []

    for target_timestamp in target_timestamps:
        # Unpack the target timestamp
        target_day, target_month, target_year, target_hour, target_minute = target_timestamp

        # Find the index where the timestamp components match the target timestamp
        index = (dataset.day == target_day) & \
                (dataset.month == target_month) & \
                (dataset.year == target_year) & \
                (dataset.hour == target_hour) & \
                (dataset.minute == target_minute)

        # If any match is found, add the index to the list
        if index.any():
            indices.append(index.nonzero()[0].item())
        else:
            indices.append(None)  # Append None if no match is found

    return indices
def compute_meteo_mean_std(dataset):
    """
    Compute mean and standard deviation for each meteo data column in the dataset.

    Parameters:
    - dataset (GHIDataset): The dataset instance

    Returns:
    - mean: Mean values for each meteo data column
    - std: Standard deviation values for each meteo data column
    """
    meteo_columns = ['day', 'month', 'year', 'hour', 'minute', 'GHI', 'air_temp', 'wind_speed', 'wind_dir', 'cosmonth']

    mean_values = []
    std_values = []

    for column in meteo_columns:
        column_data = getattr(dataset.dataset, column)
        mean_values.append(torch.mean(column_data).item())
        std_values.append(torch.std(column_data).item())

    return mean_values, std_values

def save_meteo_mean_std_to_excel(mean_values, std_values, excel_path='meteo_summary.xlsx'):
    """
    Save mean and standard deviation for each meteo data column to an Excel file.

    Parameters:
    - mean_values (list): Mean values for each meteo data column
    - std_values (list): Standard deviation values for each meteo data column
    - excel_path (str): Path to save the Excel file
    """
    summary_data = {'Column': ['day', 'month', 'year', 'hour', 'minute', 'GHI', 'air_temp', 'wind_speed', 'wind_dir', 'cosmonth'],
                    'Mean': mean_values,
                    'Std': std_values}

    summary_df = pd.DataFrame(summary_data)

    # Save the DataFrame to Excel
    summary_df.to_excel(excel_path, index=False)
    print(f"Summary saved to {excel_path}")

def plot_ghi_data_distribution(ghi_dataset):
    # Convert the dataset to a Pandas DataFrame
    df = pd.DataFrame({
        'GHI': ghi_dataset.GHI.numpy(),
        'Hour': ghi_dataset.hour.numpy(),
        'Minute': ghi_dataset.minute.numpy(),
        'Month': ghi_dataset.month.numpy(),
        'Year': ghi_dataset.year.numpy()  # Assuming you have a 'year' attribute in your GHIDataset
    })

    # Convert Hour, Minute, Month, and Year columns to integers
    df['Hour'] = df['Hour'].astype(int)
    df['Minute'] = df['Minute'].astype(int)
    df['Month'] = df['Month'].astype(int)
    df['Year'] = df['Year'].astype(int)

    # Create a new column representing both hours and minutes as a string
    df['HourMinute'] = df['Hour'].astype(str) + ':' + df['Minute'].astype(str)

    # Create a boxplot using Seaborn
    plt.figure(figsize=(18, 6))  # Adjust the figure size as needed
    sns.boxplot(x='HourMinute', y='GHI', data=df)
    plt.title('Boxplots of GHI for Each Hour and Minute')
    plt.xlabel('Hour and Minute')
    plt.ylabel('GHI')
    plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility
    plt.show()

    # Create a histogram of the number of data points per month and year
    plt.figure(figsize=(18, 6))  # Adjust the figure size as needed
    sns.histplot(x='Month', data=df, bins=12, hue='Year', multiple="stack", palette="tab10")
    plt.title('Histogram of Number of Data Points per Month and Year')
    plt.xlabel('Month')
    plt.ylabel('Count')
    plt.show()

def create_ghi_summary(ghi_dataset, output_path=None):
    """
    Create a summary DataFrame containing the hour, minute, median, Q1, and Q3 for each time.

    Parameters:
        df (pd.DataFrame): The DataFrame containing GHI data.
        output_path (str or None): The path to save the summary DataFrame as an Excel file. If None, the summary will be printed only.

    Returns:
        pd.DataFrame: The summary DataFrame.
    """
    ghi_dataset = ghi_dataset.dataset
    df = pd.DataFrame({
        'GHI': ghi_dataset.GHI.numpy(),
        'Hour': ghi_dataset.hour.numpy(),
        'Minute': ghi_dataset.minute.numpy(),
        'Month': ghi_dataset.month.numpy(),
        'Year': ghi_dataset.year.numpy()  # Assuming you have a 'year' attribute in your GHIDataset
    })

    # Convert Hour, Minute, Month, and Year columns to integers
    df['Hour'] = df['Hour'].astype(int)
    df['Minute'] = df['Minute'].astype(int)
    df['Month'] = df['Month'].astype(int)
    df['Year'] = df['Year'].astype(int)

    # Create a new column representing both hours and minutes as a string
    df['HourMinute'] = df['Hour'].astype(str) + ':' + df['Minute'].astype(str)

    # Group by hour and minute, and aggregate GHI data
    summary_df = df.groupby(['Hour', 'Minute']).agg({'GHI': ['median', 'quantile', 'quantile']}).reset_index()
    summary_df.columns = ['Hour', 'Minute', 'Median', 'Q1', 'Q3']

    # Print or use the summary DataFrame as needed
    print(summary_df)

    # Save the summary DataFrame to an Excel file if output_path is provided
    if output_path:
        summary_df.to_excel(output_path, index=False)

    return summary_df

# -------------------------------------------------------------------------------------------------------------------------------------
# Separation functions 
def split_at_location(dataset, location = 0, length = 100):
    """
      This function allows to extract the test or validation set at a given location in the dataset
        Arguments:
            location  (int) : position in the dataset where the test/validation set will start
            length    (int) : length of the test/validation set
        """
    # Initialise the indices of the dataset
    dataset_idx = np.arange(len(dataset))
    # Compute start and stop indices of the test/validation set while preventing out-of-range targets
    start_idx = max(0,location)
    end_idx = min(len(dataset),location+length)
    validation_idx = dataset_idx[start_idx:end_idx]

    # Compute the indices of the training set by removing the ones of the test/validation set.
    train_idx = np.concatenate([dataset_idx[:start_idx], dataset_idx[end_idx:]])

    # Return the two sets
    return Subset(dataset, train_idx), Subset(dataset, validation_idx)

def train_val_dataset(dataset,test_split=0.8, val_split=0.2):
    """
      split dataset in train set, validation set, test set
    """

    test_idx, train_idx = train_test_split(list(range(len(dataset))), test_size=test_split,shuffle=False)
    train_idx, val_idx = train_test_split(train_idx,test_size=val_split,shuffle=False)

    return Subset(dataset, train_idx), Subset(dataset, val_idx), Subset(dataset, test_idx)

def shuffle(dataset) :
  """
      This function allows to shuffle the data in the given set
  """

  data_idx = np.arange(len(dataset))
  np.random.shuffle(data_idx)
  #print(data_idx)
  return Subset(dataset, data_idx)
   
# -------------------------------------------------------------------------------------------------------------------------------------
